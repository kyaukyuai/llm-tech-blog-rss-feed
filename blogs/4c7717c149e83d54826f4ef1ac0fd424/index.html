<!doctype html><html lang="ja"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1"><meta name="description" content="LLMのテックブログの更新をまとめたRSSフィードを配信しています。記事を読んでその企業の技術・カルチャーを知れることや、質の高い技術情報を得られることを目的としています。"><meta name="author" content="kyaukyuai"><meta name="robots" content="index, follow"><meta property="og:url" content="https://kyaukyuai.github.io/llm-tech-blog-rss-feed/"><meta property="og:title" content="Zennの「LLM」のフィードのフィード｜LLMテックブログRSS"><meta property="og:image" content="https://kyaukyuai.github.io/llm-tech-blog-rss-feed/images/og-image.png"><meta property="og:description" content="LLMのテックブログの更新をまとめたRSSフィードを配信しています。記事を読んでその企業の技術・カルチャーを知れることや、質の高い技術情報を得られることを目的としています。"><meta property="og:type" content="website"><meta property="og:site_name" content="LLMテックブログRSS"><meta property="og:locale" content="ja_JP"><meta name="twitter:card" content="summary"><meta property="twitter:domain" 
content="https://kyaukyuai.github.io/llm-tech-blog-rss-feed/"><meta property="twitter:url" content="https://kyaukyuai.github.io/llm-tech-blog-rss-feed/"><meta name="twitter:title" content="Zennの「LLM」のフィードのフィード｜LLMテックブログRSS"><meta name="twitter:description" content="LLMのテックブログの更新をまとめたRSSフィードを配信しています。記事を読んでその企業の技術・カルチャーを知れることや、質の高い技術情報を得られることを目的としています。"><meta name="twitter:image" content="https://kyaukyuai.github.io/llm-tech-blog-rss-feed/images/og-image.png"><meta name="thumbnail" content="https://kyaukyuai.github.io/llm-tech-blog-rss-feed/images/og-image.png"><link rel="preload" href="../../styles/bundle.css" as="style"><link rel="shortcut icon" href="../../images/favicon.ico"><link rel="apple-touch-icon" href="../../images/apple-icon.png"><link rel="alternate" type="application/atom+xml" title="Atom Feed" href="../../feeds/atom.xml"><link rel="alternate" type="application/rss+xml" title="RSS2.0" href="../../feeds/rss.xml"><link rel="alternate" type="application/json" 
href="../../feeds/feed.json"><link rel="stylesheet" type="text/css" href="../../styles/bundle.css"><title>Zennの「LLM」のフィードのフィード｜LLMテックブログRSS</title></head><body><header role="banner" class="ui-section-header"><div class="ui-layout-container"><div class="ui-section-header__layout ui-layout-flex"><a href="https://kyaukyuai.github.io/llm-tech-blog-rss-feed/" role="link" aria-label="#"><img src="../../images/icon.png" alt="サイトロゴ" loading="eager" width="96" height="96"> <span class="ui-section-header__title">LLMテックブログRSS</span> </a><a href="https://github.com/kyaukyuai/llm-tech-blog-rss-feed/" role="link" aria-label="#"><img src="../../images/github-mark.png" alt="GitHubロゴ" loading="eager" width="96" height="96"></a></div></div></header><main role="main"><nav class="ui-nav"><div class="ui-layout-container"><div class="ui-section-nav__layout ui-layout-flex"><a class="ui-section-nav__link" href="../../">フィード</a> <a class="ui-section-nav__link" href="../../hot/">人気フィード</a> <a 
class="ui-section-nav__link" href="../../blogs/">ブログ一覧</a></div></div></nav><section class="ui-section-content ui-section-feed"><div class="ui-layout-container"><h2 class="ui-typography-heading">Zennの「LLM」のフィード</h2><div class="ui-container-blog-summary"><div class="ui-blog-summary"><a class="ui-blog-summary__link" href="https://zenn.dev/topics/llm">https://zenn.dev/topics/llm</a><p class="ui-blog-summary__description"></p></div></div><h3 class="ui-typography-heading">フィード</h3><div class="ui-section-content--feature ui-layout-grid ui-layout-grid-3 ui-container-feed ui-container-feed--no-image"><div class="ui-feed-item"><a class="ui-feed-item__og-image" href="https://zenn.dev/robustonian/articles/llama3_swallow_8b"><picture><source type="image/webp" srcset="../../images/feed-thumbnails/hLQpFRtgSZ-150.webp 150w, ../../images/feed-thumbnails/hLQpFRtgSZ-450.webp 450w" sizes="100vw"><img alt="記事のアイキャッチ画像" loading="lazy" decoding="async" src="../../images/feed-thumbnails/hLQpFRtgSZ-150.jpeg" 
width="450" height="236" srcset="../../images/feed-thumbnails/hLQpFRtgSZ-150.jpeg 150w, ../../images/feed-thumbnails/hLQpFRtgSZ-450.jpeg 450w" sizes="100vw"></picture></a><div class="ui-feed-item__content"><a class="ui-feed-item__title" href="https://zenn.dev/robustonian/articles/llama3_swallow_8b">MacのOllama環境でLlama-3-Swallow-8Bを使ってみた</a><div class="ui-feed-item__blog-title">Zennの「LLM」のフィード</div><div class="ui-feed-item__summary">はじめに どんな人向けの記事？ローカルLLMに興味がある方Llama-3-Swallow-8BとLlama-3-ELYZA-JP-8Bの比較をしたい方 内容Macでのollama環境構築transformerモデルからggufモデル、ollamaモデルを作成する手順Llama-3-Swallow-8Bの出力例Llama-3-ELYZA-JP-8Bとの比較本日、Llama-3-Swallowが公開されました。https://zenn.dev/tokyotech_lm/articles/f65989d76baf2c本記事では、前回の記事で購入したMac ...</div><div class="ui-feed-item__date" title="2024-07-01 14:27:02">14時間前</div></div></div><div class="ui-feed-item"><a class="ui-feed-item__og-image" href="https://zenn.dev/smartcamp/articles/b00ceecbb4e096"><picture><source type="image/webp" 
srcset="../../images/feed-thumbnails/k5CxaeHAFY-150.webp 150w, ../../images/feed-thumbnails/k5CxaeHAFY-450.webp 450w" sizes="100vw"><img alt="記事のアイキャッチ画像" loading="lazy" decoding="async" src="../../images/feed-thumbnails/k5CxaeHAFY-150.jpeg" width="450" height="236" srcset="../../images/feed-thumbnails/k5CxaeHAFY-150.jpeg 150w, ../../images/feed-thumbnails/k5CxaeHAFY-450.jpeg 450w" sizes="100vw"></picture></a><div class="ui-feed-item__content"><a class="ui-feed-item__title" href="https://zenn.dev/smartcamp/articles/b00ceecbb4e096">ZedのAssistant PanelでOllamaを使う</a><div class="ui-feed-item__blog-title">Zennの「LLM」のフィード</div><div class="ui-feed-item__summary">!筆者は LLM 周りの知識がかなり乏しいです。何か間違い・アドバイス等ありましたらコメント頂けると非常に助かります！ Zed について以下の記事で詳しく解説してます！https://zenn.dev/smartcamp/articles/c421e752119cee Assistant Panel とはVSCode のGitHub Copilot Chatのように IDE 上で生成 AI とチャットができる機能です。公式より引用詳細は公式ドキュメントをご覧ください。https://zed.dev/docs/assistant-panel Assistant...</div><div class="ui-feed-item__date" 
title="2024-07-01 14:20:10">14時間前</div></div></div><div class="ui-feed-item"><a class="ui-feed-item__og-image" href="https://zenn.dev/glavis/articles/5d8c7838d231f3"><picture><source type="image/webp" srcset="../../images/feed-thumbnails/vsBUng4Z0h-150.webp 150w, ../../images/feed-thumbnails/vsBUng4Z0h-450.webp 450w" sizes="100vw"><img alt="記事のアイキャッチ画像" loading="lazy" decoding="async" src="../../images/feed-thumbnails/vsBUng4Z0h-150.jpeg" width="450" height="236" srcset="../../images/feed-thumbnails/vsBUng4Z0h-150.jpeg 150w, ../../images/feed-thumbnails/vsBUng4Z0h-450.jpeg 450w" sizes="100vw"></picture></a><div class="ui-feed-item__content"><a class="ui-feed-item__title" href="https://zenn.dev/glavis/articles/5d8c7838d231f3">M1 MacでOllamaを使わずにPhi-3を動かしてみた話</a><div class="ui-feed-item__blog-title">Zennの「LLM」のフィード</div><div class="ui-feed-item__summary">
概要Phi-3に続きGemma2がリリースされるなど、ローカルで動かせるLLMが増えてきているのでキャッチアップのために触ってみようと思い、M1 Macを使って色々試したお話。Ollamaでやれば瞬殺だったが、瞬殺すぎたので別の方法でも動かしてみた。 tl;drmlx-communityが提供しているモデルの利用方法に従えばすぐに動く。 まずはOllamaで動かしてみるOllamaのGitHub Repoを確認すると色々説明が書いてあるので都合が良い方法でセットアップを進める。https://github.com/ollama/ollama今回は直接インストールす...</div><div class="ui-feed-item__date" title="2024-07-01 11:46:12">16時間前</div></div></div><div class="ui-feed-item"><a class="ui-feed-item__og-image" href="https://zenn.dev/devopsinqa/articles/9d4b2e7e99474e"><picture><source type="image/webp" srcset="../../images/feed-thumbnails/GiQ8CUnFIj-150.webp 150w, ../../images/feed-thumbnails/GiQ8CUnFIj-450.webp 450w" sizes="100vw"><img alt="記事のアイキャッチ画像" loading="lazy" decoding="async" src="../../images/feed-thumbnails/GiQ8CUnFIj-150.jpeg" width="450" height="236" srcset="../../images/feed-thumbnails/GiQ8CUnFIj-150.jpeg 150w, ../../images/feed-thumbnails/GiQ8CUnFIj-450.jpeg 450w" sizes="100vw"></picture></a><div class="ui-feed-item__content"><a 
class="ui-feed-item__title" href="https://zenn.dev/devopsinqa/articles/9d4b2e7e99474e">モデルトリック LLMにおけるQA LLMにおけるQAで抑えておきたいキーワード</a><div class="ui-feed-item__blog-title">Zennの「LLM」のフィード</div><div class="ui-feed-item__summary">モデルトリックとはLLM（Large Language Models）における「モデルトリック」は、モデルが意図的にプログラムされた動作や制約を回避したり、予期しない方法で動作することを指します。モデルトリックは、品質保証（QA）およびセキュリティの観点からいくつかのリスクをもたらします。以下に詳しく説明します。 QA（品質保証）の観点からのモデルトリック不安定な出力モデルトリックによって、モデルは不安定な出力を生成する可能性があります。これは、一貫性のある品質の維持が困難になるため、ユーザー体験に悪影響を与える可能性があります。テストカバレッジの欠如モデ...</div><div class="ui-feed-item__date" title="2024-07-01 10:03:00">18時間前</div></div></div><div class="ui-feed-item"><a class="ui-feed-item__og-image" href="https://zenn.dev/datasciencekun/articles/bcf1c2afcbe91a"><picture><source type="image/webp" srcset="../../images/feed-thumbnails/bt46MI_9sc-150.webp 150w, ../../images/feed-thumbnails/bt46MI_9sc-450.webp 450w" sizes="100vw"><img alt="記事のアイキャッチ画像" loading="lazy" decoding="async" src="../../images/feed-thumbnails/bt46MI_9sc-150.jpeg" 
width="450" height="236" srcset="../../images/feed-thumbnails/bt46MI_9sc-150.jpeg 150w, ../../images/feed-thumbnails/bt46MI_9sc-450.jpeg 450w" sizes="100vw"></picture></a><div class="ui-feed-item__content"><a class="ui-feed-item__title" href="https://zenn.dev/datasciencekun/articles/bcf1c2afcbe91a">OllamaでOpen LLMをローカルでデプロイ</a><div class="ui-feed-item__blog-title">Zennの「LLM」のフィード</div><div class="ui-feed-item__summary">概要使えるOpen LLMをチェックOllamaでOpen LLMをローカルでデプロイWeb UIを動かすAPIで呼び出し ハードウェアMacBook Pro 2021メモリ：16GBチップ：Apple M1 Pro 使えるモデル一覧 Open LLM leaderboardhttps://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard デプロイできるモデル一覧https://ollama.com/library 必要なメモリ円滑にモデルを動かすために、必要...</div><div class="ui-feed-item__date" title="2024-07-01 09:07:28">19時間前</div></div></div><div class="ui-feed-item"><a class="ui-feed-item__og-image" href="https://zenn.dev/turing_motors/articles/8c95f6c656dcf8"><picture><source type="image/webp" 
srcset="../../images/feed-thumbnails/2X2OVx77VN-150.webp 150w, ../../images/feed-thumbnails/2X2OVx77VN-450.webp 450w" sizes="100vw"><img alt="記事のアイキャッチ画像" loading="lazy" decoding="async" src="../../images/feed-thumbnails/2X2OVx77VN-150.jpeg" width="450" height="236" srcset="../../images/feed-thumbnails/2X2OVx77VN-150.jpeg 150w, ../../images/feed-thumbnails/2X2OVx77VN-450.jpeg 450w" sizes="100vw"></picture></a><div class="ui-feed-item__content"><a class="ui-feed-item__title" href="https://zenn.dev/turing_motors/articles/8c95f6c656dcf8">第3世代の自動運転@CVPR2024</a><div class="ui-feed-item__hatena-count" title="はてなブックマーク数"><img src="../../images/hatenabookmark-icon.png" alt="はてなブックマークアイコン" loading="lazy" width="96" height="96"> <span>1</span></div><div class="ui-feed-item__blog-title">Zennの「LLM」のフィード</div><div class="ui-feed-item__summary">
はじめにTuring 生成AIチームの佐々木 (kento_sasaki1)です。生成AIチームでは、完全自動運転の実現に向けてマルチモーダル基盤モデルの開発に取り組んでいます。先日、6月17日から6月21日にシアトルで開催されたコンピュータビジョン・機械学習系のトップカンファレンスCVPR 2024に参加し、Vision Language Model (VLM)のワークショップThe 3rd Workshop on Computer Vision in the Wildにて日本語VLM評価ベンチマークHeron-Benchの発表を行いました。Heron-Benchについては...</div><div class="ui-feed-item__date" title="2024-07-01 08:00:01">20時間前</div></div></div><div class="ui-feed-item"><a class="ui-feed-item__og-image" href="https://zenn.dev/tokyotech_lm/articles/f65989d76baf2c"><picture><source type="image/webp" srcset="../../images/feed-thumbnails/K-2RL4kfjS-150.webp 150w, ../../images/feed-thumbnails/K-2RL4kfjS-450.webp 450w" sizes="100vw"><img alt="記事のアイキャッチ画像" loading="lazy" decoding="async" src="../../images/feed-thumbnails/K-2RL4kfjS-150.jpeg" width="450" height="236" srcset="../../images/feed-thumbnails/K-2RL4kfjS-150.jpeg 150w, ../../images/feed-thumbnails/K-2RL4kfjS-450.jpeg 450w" sizes="100vw"></picture></a><div class="ui-feed-item__content"><a 
class="ui-feed-item__title" href="https://zenn.dev/tokyotech_lm/articles/f65989d76baf2c">Llama-3-Swallow: 日本語に強い継続事前学習モデル</a><div class="ui-feed-item__hatena-count" title="はてなブックマーク数"><img src="../../images/hatenabookmark-icon.png" alt="はてなブックマークアイコン" loading="lazy" width="96" height="96"> <span>3</span></div><div class="ui-feed-item__blog-title">Zennの「LLM」のフィード</div><div class="ui-feed-item__summary">はじめに東京工業大学の藤井です。本日(2024/07/01) Meta-Llama-3-8BとMeta-Llama-3-70Bから日本語を中心としたコーパスで継続事前学習を行ったLlama-3-Swallow-8B-v0.1、Llama-3-Swallow-70B-v0.1とそのinstructモデルであるLlama-3-Swallow-8B-instruct-v0.1、Llama-3-Swallow-70B-instruct-v0.1をリリースさせていただきました。このモデルはMeta社のライセンスを踏襲しており、商用利用が可能です。本モデルの開発は、産総研、東京工業大学 ...</div><div class="ui-feed-item__date" title="2024-07-01 06:55:00">21時間前</div></div></div><div class="ui-feed-item"><a class="ui-feed-item__og-image" href="https://zenn.dev/robustonian/articles/selection_of_gpus_for_local_llm"><picture><source type="image/webp" 
srcset="../../images/feed-thumbnails/Jdw1G1ilvN-150.webp 150w, ../../images/feed-thumbnails/Jdw1G1ilvN-450.webp 450w" sizes="100vw"><img alt="記事のアイキャッチ画像" loading="lazy" decoding="async" src="../../images/feed-thumbnails/Jdw1G1ilvN-150.jpeg" width="450" height="236" srcset="../../images/feed-thumbnails/Jdw1G1ilvN-150.jpeg 150w, ../../images/feed-thumbnails/Jdw1G1ilvN-450.jpeg 450w" sizes="100vw"></picture></a><div class="ui-feed-item__content"><a class="ui-feed-item__title" href="https://zenn.dev/robustonian/articles/selection_of_gpus_for_local_llm">ローカルLLMのための最適なGPU選定：Mac Studio購入の決め手</a><div class="ui-feed-item__blog-title">Zennの「LLM」のフィード</div><div class="ui-feed-item__summary">はじめに どんな人向けの記事？ローカルLLMに興味がある方ローカルLLM向けにGPUの購入を検討している方 内容本記事では、ローカルLLMの推論に向いているGPUの選定を行います。タイトルにある通り、結論から言うと私はM2 Ultra搭載のMac Studioを購入しました。なぜ私が60万円以上もするMac Studioを購入するに至ったのか？その経緯を書き留めています。 検討項目ローカルLLMの推論向けに最適なGPUは何か？今後のGPU販売計画も考慮して、今買うべきか待つべきか？Mac Studioを購入する際の具体的なスペックはどうすべきか？...</div><div 
class="ui-feed-item__date" title="2024-07-01 02:39:14">1日前</div></div></div><div class="ui-feed-item"><a class="ui-feed-item__og-image" href="https://zenn.dev/sterashima78/articles/61c346b3c261fa"><picture><source type="image/webp" srcset="../../images/feed-thumbnails/qeNUHAeo1Y-150.webp 150w, ../../images/feed-thumbnails/qeNUHAeo1Y-450.webp 450w" sizes="100vw"><img alt="記事のアイキャッチ画像" loading="lazy" decoding="async" src="../../images/feed-thumbnails/qeNUHAeo1Y-150.jpeg" width="450" height="236" srcset="../../images/feed-thumbnails/qeNUHAeo1Y-150.jpeg 150w, ../../images/feed-thumbnails/qeNUHAeo1Y-450.jpeg 450w" sizes="100vw"></picture></a><div class="ui-feed-item__content"><a class="ui-feed-item__title" href="https://zenn.dev/sterashima78/articles/61c346b3c261fa">静的サイトに対する検索にWebブラウザで動作する LLM の利用を試す</a><div class="ui-feed-item__hatena-count" title="はてなブックマーク数"><img src="../../images/hatenabookmark-icon.png" alt="はてなブックマークアイコン" loading="lazy" width="96" height="96"> <span>2</span></div>
<div class="ui-feed-item__blog-title">Zennの「LLM」のフィード</div><div class="ui-feed-item__summary">はじめにライブラリのドキュメントサイトなどはコンテンツが静的であるため、GitHub Pages など安価なサービスを利用して、静的サイトとして実装することが多い。一方で、利用者が求める情報にいち早く到達するためには、静的コンテンツに対する検索機能が有効だが、これは動的な機能だ。そこで、pagefind などは、バックエンドサーバを持たずに静的なサイトを検索する機能を提供してくれる。これらはビルドされた静的コンテンツからインデックスデータを生成し、これをサイトと一緒にデプロイすることで、利用者の環境で検索を実行する。この機能を利用することで、ユーザは自身の興味のある情報が掲載...</div><div class="ui-feed-item__date" title="2024-06-30 12:30:58">2日前</div></div></div><div class="ui-feed-item"><a class="ui-feed-item__og-image" href="https://zenn.dev/randd_inc/articles/d65236540c7060"><picture><source type="image/webp" srcset="../../images/feed-thumbnails/523wmj2YHz-150.webp 150w, ../../images/feed-thumbnails/523wmj2YHz-450.webp 450w" sizes="100vw"><img alt="記事のアイキャッチ画像" loading="lazy" decoding="async" src="../../images/feed-thumbnails/523wmj2YHz-150.jpeg" width="450" height="236" 
srcset="../../images/feed-thumbnails/523wmj2YHz-150.jpeg 150w, ../../images/feed-thumbnails/523wmj2YHz-450.jpeg 450w" sizes="100vw"></picture></a><div class="ui-feed-item__content"><a class="ui-feed-item__title" href="https://zenn.dev/randd_inc/articles/d65236540c7060">小説家全員猫にしてみた</a><div class="ui-feed-item__blog-title">Zennの「LLM」のフィード</div><div class="ui-feed-item__summary">はじめに畑田です。LangChainを使って青空文庫の文章を猫にしてみました。 コード以下が実際に書いたコードです。めっちゃ短いです。import { config } from &#39;dotenv&#39;import { ChatGoogleGenerativeAI } from &#39;@langchain/google-genai&#39;import { HarmBlockThreshold, HarmCategory } from &#39;@google/generative-ai&#39;import { createInterface } from &#39;readline/promises&#39;...</div><div class="ui-feed-item__date" title="2024-06-30 07:53:47">2日前</div></div></div><div class="ui-feed-item"><a class="ui-feed-item__og-image" href="https://zenn.dev/xiakou/articles/6ca3cfb377a7c3"><picture><source type="image/webp" 
srcset="../../images/feed-thumbnails/gODov-R-RL-150.webp 150w, ../../images/feed-thumbnails/gODov-R-RL-450.webp 450w" sizes="100vw"><img alt="記事のアイキャッチ画像" loading="lazy" decoding="async" src="../../images/feed-thumbnails/gODov-R-RL-150.jpeg" width="450" height="236" srcset="../../images/feed-thumbnails/gODov-R-RL-150.jpeg 150w, ../../images/feed-thumbnails/gODov-R-RL-450.jpeg 450w" sizes="100vw"></picture></a><div class="ui-feed-item__content"><a class="ui-feed-item__title" href="https://zenn.dev/xiakou/articles/6ca3cfb377a7c3">【n番煎じ】ローカルDifyでLLMと遊ぶ【無料】</a><div class="ui-feed-item__blog-title">Zennの「LLM」のフィード</div><div class="ui-feed-item__summary">ローカルDifyでLLMと遊ぶ in Linux はじめにLinux環境でDifyのサーバを立ち上げ，LLMと遊ぶ環境を構築します．DifyはオープンソースのLLMアプリ開発アプリケーションです．チャットボットや論文要約アプリなどが手軽に開発できます．今回は手元のLinuxマシンで立ち上げたサーバに対してWindows PCからアクセスすることを想定しています． 環境構築例今回構築する環境は以下の通りです．OS：Linux（Ubuntu 22.0.4）LLM：Groq（2024年6月現在はβ版のため無料）サーバ：適当なLinux OSのマシン（今回はRaspb...</div><div class="ui-feed-item__date" 
title="2024-06-30 06:55:17">2日前</div></div></div><div class="ui-feed-item"><a class="ui-feed-item__og-image" href="https://zenn.dev/michy/articles/1dfddbcf88c80e"><picture><source type="image/webp" srcset="../../images/feed-thumbnails/QrOYLVG6Lx-150.webp 150w, ../../images/feed-thumbnails/QrOYLVG6Lx-450.webp 450w" sizes="100vw"><img alt="記事のアイキャッチ画像" loading="lazy" decoding="async" src="../../images/feed-thumbnails/QrOYLVG6Lx-150.jpeg" width="450" height="236" srcset="../../images/feed-thumbnails/QrOYLVG6Lx-150.jpeg 150w, ../../images/feed-thumbnails/QrOYLVG6Lx-450.jpeg 450w" sizes="100vw"></picture></a><div class="ui-feed-item__content"><a class="ui-feed-item__title" href="https://zenn.dev/michy/articles/1dfddbcf88c80e">DifyとOllamaを使ってMacのローカルで動く業務用LLM-AIツールを作成する</a><div class="ui-feed-item__hatena-count" title="はてなブックマーク数"><img src="../../images/hatenabookmark-icon.png" alt="はてなブックマークアイコン" loading="lazy" width="96" height="96"> <span>3</span></div><div 
class="ui-feed-item__blog-title">Zennの「LLM」のフィード</div><div class="ui-feed-item__summary">はじめにこの記事はDifyを使った業務用AIツールの開発方法をまとめたものです。Macのローカル環境上でテキスト分類器を作成することをゴールとしており、環境構築から実装までを初心者向けに最初から解説しています。 環境構築 モデルの入手モデルは商用利用可能なElyzaを利用します。公式が量子化(軽量化)済みのGGUF版を提供してくださっているのでそちらを使用します。以下のURLのFiles and Versionsからggufファイルをダウンロードしておいてください。https://huggingface.co/elyza/Llama-3-ELYZA-JP-8B-...</div><div class="ui-feed-item__date" title="2024-06-30 04:41:49">2日前</div></div></div><div class="ui-feed-item"><a class="ui-feed-item__og-image" href="https://zenn.dev/yutech_kyocha/articles/45f93e43c2caa6"><picture><source type="image/webp" srcset="../../images/feed-thumbnails/URKkKqqDA4-150.webp 150w, ../../images/feed-thumbnails/URKkKqqDA4-450.webp 450w" sizes="100vw"><img alt="記事のアイキャッチ画像" loading="lazy" decoding="async" src="../../images/feed-thumbnails/URKkKqqDA4-150.jpeg" width="450" height="236" 
srcset="../../images/feed-thumbnails/URKkKqqDA4-150.jpeg 150w, ../../images/feed-thumbnails/URKkKqqDA4-450.jpeg 450w" sizes="100vw"></picture></a><div class="ui-feed-item__content"><a class="ui-feed-item__title" href="https://zenn.dev/yutech_kyocha/articles/45f93e43c2caa6">LLMや生成AIに関する動向のまとめ(2023/6〜2024/6)</a><div class="ui-feed-item__blog-title">Zennの「LLM」のフィード</div><div class="ui-feed-item__summary">LLM/GenAIの動向まとめ2023/6-2024/6以下の記事は生成AIによる文章生成を含むため、誤った内容を含む場合があります。 最新の発表されたモデルの情報 期間：2023/6/1～2024/6/30モデルの名前発表機関発表年月日モデルのサイズモデルの概要モデルの特徴新しくできるようになったこと記載されたサイトや論文のタイトルURLFalcon 180BTechnology Innovation Institute (TII)2024/06/01180 billionFalcon 40Bのアップグレード版。様々なタスク（推論...</div><div class="ui-feed-item__date" title="2024-06-30 01:19:41">2日前</div></div></div><div class="ui-feed-item"><a class="ui-feed-item__og-image" href="https://zenn.dev/narumin256/articles/34f85f7a339d86"><picture><source type="image/webp" 
srcset="../../images/feed-thumbnails/EkASGyDOic-150.webp 150w, ../../images/feed-thumbnails/EkASGyDOic-450.webp 450w" sizes="100vw"><img alt="記事のアイキャッチ画像" loading="lazy" decoding="async" src="../../images/feed-thumbnails/EkASGyDOic-150.jpeg" width="450" height="236" srcset="../../images/feed-thumbnails/EkASGyDOic-150.jpeg 150w, ../../images/feed-thumbnails/EkASGyDOic-450.jpeg 450w" sizes="100vw"></picture></a><div class="ui-feed-item__content"><a class="ui-feed-item__title" href="https://zenn.dev/narumin256/articles/34f85f7a339d86">ディープラーニング分野で話題のモデル「Transformer」って何がすごいの？📚</a><div class="ui-feed-item__blog-title">Zennの「LLM」のフィード</div><div class="ui-feed-item__summary">こんにちは！テクニカルディレクター集団「BASSDRUM」のnaruminです。先日AIの歴史や技術の変遷について社内勉強会を行いました。その時に紹介された「Transformer」というディープラーニングのモデルが何だかすごいということを知り、数理的な理解は難しくとも凄さを理解するために調べてみました。「Transformerって聞いたことあるけど一体何？」と思ってる方に役立てると嬉しいです。 1. TransformerモデルとはTransformerは、2017年にGoogleの研究者らによって発表された機械学習モデルで、自然言語処理（NLP）の分野で革命を起こしました。そ...</div><div 
class="ui-feed-item__date" title="2024-06-29 13:32:10">3日前</div></div></div><div class="ui-feed-item"><a class="ui-feed-item__og-image" href="https://zenn.dev/intel_developer/articles/e92263073797cf"><picture><source type="image/webp" srcset="../../images/feed-thumbnails/zeCafFhIsl-150.webp 150w, ../../images/feed-thumbnails/zeCafFhIsl-450.webp 450w" sizes="100vw"><img alt="記事のアイキャッチ画像" loading="lazy" decoding="async" src="../../images/feed-thumbnails/zeCafFhIsl-150.jpeg" width="450" height="236" srcset="../../images/feed-thumbnails/zeCafFhIsl-150.jpeg 150w, ../../images/feed-thumbnails/zeCafFhIsl-450.jpeg 450w" sizes="100vw"></picture></a><div class="ui-feed-item__content"><a class="ui-feed-item__title" href="https://zenn.dev/intel_developer/articles/e92263073797cf">AI PC で夢を描く</a><div class="ui-feed-item__blog-title">Zennの「LLM」のフィード</div><div class="ui-feed-item__summary">
この記事は、Medium に公開されている「Paint Your Dream with AI PC」の日本語参考訳です。原文は更新される可能性があります。原文と翻訳文の内容が異なる場合は原文を優先してください。https://medium.com/openvino-toolkit/paint-your-dream-with-ai-pc-e0c35cbeca9fこの記事の PDF 版は下記からご利用になれます。https://www.isus.jp/wp-content/uploads/pdf/openvino_28_paint-your-dream-with-ai-pc.pdf...</div><div class="ui-feed-item__date" title="2024-06-29 12:37:51">3日前</div></div></div><div class="ui-feed-item"><a class="ui-feed-item__og-image" href="https://zenn.dev/intel_developer/articles/82d31988c11e45"><picture><source type="image/webp" srcset="../../images/feed-thumbnails/q8rMZs65hy-150.webp 150w, ../../images/feed-thumbnails/q8rMZs65hy-450.webp 450w" sizes="100vw"><img alt="記事のアイキャッチ画像" loading="lazy" decoding="async" src="../../images/feed-thumbnails/q8rMZs65hy-150.jpeg" width="450" height="236" srcset="../../images/feed-thumbnails/q8rMZs65hy-150.jpeg 150w, ../../images/feed-thumbnails/q8rMZs65hy-450.jpeg 450w" sizes="100vw"></picture></a><div class="ui-feed-item__content"><a 
class="ui-feed-item__title" href="https://zenn.dev/intel_developer/articles/82d31988c11e45">LangChain が OpenVINO™ を公式にサポート</a><div class="ui-feed-item__blog-title">Zennの「LLM」のフィード</div><div class="ui-feed-item__summary">この記事は、Medium に公開されている「LangChain Officially Supports OpenVINO™ Now!」の日本語参考訳です。原文は更新される可能性があります。原文と翻訳文の内容が異なる場合は原文を優先してください。https://medium.com/openvino-toolkit/langchain-officially-supports-openvino-now-a88f75793d84この記事の PDF 版は下記URLからご利用になれます。https://www.isus.jp/wp-content/uploads/pdf/openvino_2...</div><div class="ui-feed-item__date" title="2024-06-29 11:34:46">3日前</div></div></div><div class="ui-feed-item"><a class="ui-feed-item__og-image" href="https://zenn.dev/ino_h/articles/2024-06-29-llm-prompt-doc"><picture><source type="image/webp" srcset="../../images/feed-thumbnails/VVDaU_qsSp-150.webp 150w, ../../images/feed-thumbnails/VVDaU_qsSp-450.webp 450w" sizes="100vw"><img alt="記事のアイキャッチ画像" loading="lazy" decoding="async" 
src="../../images/feed-thumbnails/VVDaU_qsSp-150.jpeg" width="450" height="236" srcset="../../images/feed-thumbnails/VVDaU_qsSp-150.jpeg 150w, ../../images/feed-thumbnails/VVDaU_qsSp-450.jpeg 450w" sizes="100vw"></picture></a><div class="ui-feed-item__content"><a class="ui-feed-item__title" href="https://zenn.dev/ino_h/articles/2024-06-29-llm-prompt-doc">Anthropic のプロンプトジェネレータを使ってみた</a><div class="ui-feed-item__blog-title">Zennの「LLM」のフィード</div><div class="ui-feed-item__summary">Anthropic のプロンプトエンジニアリングを重点に学ぶため https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/overview を読んでみようと思います。 ガイドラインの前提参考のサイトは下記を前提としたドキュメントになっています。ユースケースの成功基準を明確に定義するこれらの基準を経験的にテストする方法改善したい最初のドラフトプロンプトもし前提に満たない場合は 成功基準の定義 や 強力な実証的評価の作成 を読むとよいとのこと。 プロンプトエンジニアリング手法プロン...</div><div class="ui-feed-item__date" title="2024-06-29 04:24:48">3日前</div></div></div><div class="ui-feed-item"><a class="ui-feed-item__og-image" href="https://zenn.dev/coji/articles/vercel-ai-sdk-use-object-is-nice"><picture>
<source type="image/webp" srcset="../../images/feed-thumbnails/47v51RXmak-150.webp 150w, ../../images/feed-thumbnails/47v51RXmak-450.webp 450w" sizes="100vw"><img alt="記事のアイキャッチ画像" loading="lazy" decoding="async" src="../../images/feed-thumbnails/47v51RXmak-150.jpeg" width="450" height="236" srcset="../../images/feed-thumbnails/47v51RXmak-150.jpeg 150w, ../../images/feed-thumbnails/47v51RXmak-450.jpeg 450w" sizes="100vw"></picture></a><div class="ui-feed-item__content"><a class="ui-feed-item__title" href="https://zenn.dev/coji/articles/vercel-ai-sdk-use-object-is-nice">Vercel AI SDK useObject: リッチなストリーミングUIを簡単に</a><div class="ui-feed-item__hatena-count" title="はてなブックマーク数"><img src="../../images/hatenabookmark-icon.png" alt="はてなブックマークアイコン" loading="lazy" width="96" height="96"> <span>1</span></div><div class="ui-feed-item__blog-title">Zennの「LLM」のフィード</div><div class="ui-feed-item__summary">
Vercel AI SDK useObject: 効率的なLLMストリーミング処理の実現Vercel AI SDKのuseObjectフックを使用した経験から、その有用性と開発効率の向上について共有します。 useObjectの概要useObjectは、Vercel AI SDKが提供するReactフックです。このフックを使用することで、AIからのストリーミングレスポンスをJSONオブジェクトとして簡単に扱うことができ、それを使ってリッチなストリーミングUIを簡単に作ることができます。たとえば、以下のような UIを 10分で作ることができます。 主な特徴使用の簡...</div><div class="ui-feed-item__date" title="2024-06-28 11:37:53">4日前</div></div></div><div class="ui-feed-item"><a class="ui-feed-item__og-image" href="https://zenn.dev/cloud_ace/articles/llm-sugests-lol-meta-builds"><picture><source type="image/webp" srcset="../../images/feed-thumbnails/woYlmuKAG1-150.webp 150w, ../../images/feed-thumbnails/woYlmuKAG1-450.webp 450w" sizes="100vw"><img alt="記事のアイキャッチ画像" loading="lazy" decoding="async" src="../../images/feed-thumbnails/woYlmuKAG1-150.jpeg" width="450" height="236" srcset="../../images/feed-thumbnails/woYlmuKAG1-150.jpeg 150w, ../../images/feed-thumbnails/woYlmuKAG1-450.jpeg 450w" sizes="100vw"></picture></a><div class="ui-feed-item__content"><a 
class="ui-feed-item__title" href="https://zenn.dev/cloud_ace/articles/llm-sugests-lol-meta-builds">AI に League of Legends の最強新メタビルドを考えてもらいたかった</a><div class="ui-feed-item__blog-title">Zennの「LLM」のフィード</div><div class="ui-feed-item__summary">TL;DRAI に League of Legends のビルドを考えてもらって実践したらボコボコに負けました。 対象読者League of Legends をプレイしている人LLM に興味がある人Gemini 1.5 Pro に興味がある人LLM で大きいデータを楽に扱いたい人 サモナーズリフトへようこそこんにちは。クラウドエース バックエンドエンジニアリング部 の伊藝です。みなさんは、League of Legends をプレイされていますか？League of Legends (LoL) とは、Riot Games が開発した 5 対 5 のマルチプ...</div><div class="ui-feed-item__date" title="2024-06-28 08:12:23">4日前</div></div></div><div class="ui-feed-item"><a class="ui-feed-item__og-image" href="https://zenn.dev/happy_elements/articles/cc3eb400486214"><picture><source type="image/webp" srcset="../../images/feed-thumbnails/TL5h0LVBo_-150.webp 150w, ../../images/feed-thumbnails/TL5h0LVBo_-450.webp 450w" sizes="100vw"><img alt="記事のアイキャッチ画像" loading="lazy" decoding="async" 
src="../../images/feed-thumbnails/TL5h0LVBo_-150.jpeg" width="450" height="236" srcset="../../images/feed-thumbnails/TL5h0LVBo_-150.jpeg 150w, ../../images/feed-thumbnails/TL5h0LVBo_-450.jpeg 450w" sizes="100vw"></picture></a><div class="ui-feed-item__content"><a class="ui-feed-item__title" href="https://zenn.dev/happy_elements/articles/cc3eb400486214">Difyの活用について</a><div class="ui-feed-item__hatena-count" title="はてなブックマーク数"><img src="../../images/hatenabookmark-icon.png" alt="はてなブックマークアイコン" loading="lazy" width="96" height="96"> <span>1</span></div><div class="ui-feed-item__blog-title">Zennの「LLM」のフィード</div><div class="ui-feed-item__summary">Difyを導入しました弊社では主にデータ分析やプログラミングの効率化などでLLMを活用しています。GPT-4が出て4ヶ月後には誰でも利用可能な形でチャット機能をリリースするなど環境整備を行ってきました。当時はフロントエンドとして以下のOSSを使いLLMはAzure OpenAI Serviceとつなぎ、RAGは自前のサーバーで実現していました。https://github.com/ChatGPTNextWeb/ChatGPT-Next-Webしかしながら、LLMの進歩も著しく最先端モデルや各種周辺サービスの発展速度に対して、画像チャット機能の導入やClaude3やGemin...</div><div class="ui-feed-item__date" 
title="2024-06-28 04:28:43">4日前</div></div></div></div></div></section></main><footer role="contentinfo" class="ui-section-footer"><div class="ui-layout-container"><div class="ui-layout-column-6 ui-layout-column-center"><div class="ui-component-cta ui-layout-flex ui-section-footer__site-info"><p class="ui-text-note">このサイトは<br>記事を読んでその企業の技術・カルチャーを知れることや<br>質の高い技術情報を得られることを目的としています。</p><p class="ui-text-note">追加したいブログがある場合は<br><a href="https://github.com/kyaukyuai/llm-tech-blog-rss-feed#%E3%82%B5%E3%82%A4%E3%83%88%E3%81%AE%E8%BF%BD%E5%8A%A0%E6%96%B9%E6%B3%95">サイトの追加方法</a> をご参照ください。</p></div></div></div><div class="ui-layout-container"><div class="ui-section-footer__layout ui-layout-flex"><p class="ui-section-footer--copyright ui-text-note"><a class="ui-text-note" href="https://github.com/kyaukyuai/"><small>@kyaukyuai</small></a></p><a href="https://github.com/kyaukyuai/llm-tech-blog-rss-feed/" role="link" aria-label="#" class="ui-text-note"><small>GitHub</small></a></div></div></footer>
</body></html>